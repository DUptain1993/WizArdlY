{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Fine-Tuning Uncensored CodeGemma 7B with GCS Streaming for Kaggle\n",
    "\n",
    "## Overview\n",
    "Fine-tunes `ICEPVP8977/Uncensored_codegemma_7b` using all specified HF datasets (18 sources merged). Uses Unsloth + 4-bit for low VRAM (~4-6GB). **Streams all datasets and models to/from GCS buckets** for Kaggle compatibility.\n",
    "\n",
    "**Datasets:** Full merge (~500k-700k examples, ~1.5-2.5GB); streamed from GCS.\n",
    "**Storage:** All data streamed to/from GCS buckets - no local storage limits.\n",
    "**Time:** 1-2h sampled / 4-8h full on T4 GPU.\n",
    "**Output:** Streamed to GCS bucket as `fine_tuned_model_f16.gguf` (~14GB).\n",
    "\n",
    "**GCS Buckets:**\n",
    "- Models: `gs://wizard-coder-ai-models-1759403941/`\n",
    "- Datasets: `gs://wizard-coder-datasets-1759403954/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies (Unsloth-Optimized + GCS Streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth for 2-5x faster, 80% less VRAM\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q xformers trl peft accelerate bitsandbytes\n",
    "!pip install -q transformers==4.44.2 datasets==2.21.0 huggingface_hub==0.24.6 safetensors==0.4.5\n",
    "\n",
    "# GCS streaming support\n",
    "!pip install -q google-cloud-storage gcsfs\n",
    "\n",
    "# GGUF conversion\n",
    "!pip install -q llama-cpp-python\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git /tmp/llama.cpp 2>/dev/null || echo \"llama.cpp already cloned\"\n",
    "%cd /tmp/llama.cpp\n",
    "!make clean && make -j\n",
    "%cd /kaggle/working\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import os\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "!nvidia-smi\n",
    "!df -h  # Disk usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup GCS Authentication and Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "import os\n",
    "\n",
    "# GCS Configuration\n",
    "MODELS_BUCKET = 'wizard-coder-ai-models-1759403941'\n",
    "DATASETS_BUCKET = 'wizard-coder-datasets-1759403954'\n",
    "PROJECT_ID = 'wizardlm-vertex-1759276927'\n",
    "\n",
    "# Authenticate with GCS (using Kaggle's built-in auth or service account)\n",
    "try:\n",
    "    # Try using default credentials first\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    print(f\"✅ Authenticated with GCS project: {PROJECT_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ GCS auth failed: {e}\")\n",
    "    print(\"Please ensure you're running in Kaggle with GCS access enabled\")\n",
    "    # Alternative: Use service account key if uploaded as Kaggle dataset\n",
    "    # os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/your-key-dataset/key.json'\n",
    "    # client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "# Verify buckets exist\n",
    "try:\n",
    "    models_bucket = client.bucket(MODELS_BUCKET)\n",
    "    datasets_bucket = client.bucket(DATASETS_BUCKET)\n",
    "    print(f\"✅ Models bucket: gs://{MODELS_BUCKET}\")\n",
    "    print(f\"✅ Datasets bucket: gs://{DATASETS_BUCKET}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Bucket access failed: {e}\")\n",
    "    print(\"Creating buckets...\")\n",
    "    try:\n",
    "        models_bucket = client.create_bucket(MODELS_BUCKET)\n",
    "        datasets_bucket = client.create_bucket(DATASETS_BUCKET)\n",
    "        print(\"✅ Buckets created successfully\")\n",
    "    except Exception as create_e:\n",
    "        print(f\"❌ Bucket creation failed: {create_e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Login to Hugging Face (for Gated Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Paste HF token (e.g., hf_XXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GCS Streaming Dataset Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import io\n",
    "\n",
    "class GCSDatasetManager:\n",
    "    def __init__(self, client, datasets_bucket_name):\n",
    "        self.client = client\n",
    "        self.datasets_bucket = client.bucket(datasets_bucket_name)\n",
    "    \n",
    "    def save_dataset_to_gcs(self, dataset, name):\n",
    "        \"\"\"Save dataset to GCS bucket\"\"\"\n",
    "        print(f\"📤 Uploading dataset '{name}' to GCS...\")\n",
    "        \n",
    "        # Create temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            local_path = os.path.join(temp_dir, name)\n",
    "            dataset.save_to_disk(local_path)\n",
    "            \n",
    "            # Upload all files to GCS\n",
    "            for root, dirs, files in os.walk(local_path):\n",
    "                for file in files:\n",
    "                    local_file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(local_file_path, temp_dir)\n",
    "                    gcs_path = f\"{name}/{relative_path}\"\n",
    "                    \n",
    "                    blob = self.datasets_bucket.blob(gcs_path)\n",
    "                    blob.upload_from_filename(local_file_path)\n",
    "                    \n",
    "        print(f\"✅ Dataset '{name}' uploaded to gs://{self.datasets_bucket.name}/{name}/\")\n",
    "    \n",
    "    def load_dataset_from_gcs(self, name):\n",
    "        \"\"\"Load dataset from GCS bucket\"\"\"\n",
    "        print(f\"📥 Downloading dataset '{name}' from GCS...\")\n",
    "        \n",
    "        # Create temporary directory\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        local_path = os.path.join(temp_dir, name)\n",
    "        \n",
    "        # Download all files from GCS\n",
    "        blobs = self.datasets_bucket.list_blobs(prefix=f\"{name}/\")\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('/'):  # Skip directory markers\n",
    "                continue\n",
    "                \n",
    "            relative_path = blob.name[len(f\"{name}/\"):]\n",
    "            local_file_path = os.path.join(local_path, relative_path)\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            \n",
    "            blob.download_to_filename(local_file_path)\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = Dataset.load_from_disk(local_path)\n",
    "        \n",
    "        # Cleanup\n",
    "        shutil.rmtree(temp_dir)\n",
    "        \n",
    "        print(f\"✅ Dataset '{name}' loaded from GCS\")\n",
    "        return dataset\n",
    "    \n",
    "    def dataset_exists_in_gcs(self, name):\n",
    "        \"\"\"Check if dataset exists in GCS\"\"\"\n",
    "        blobs = list(self.datasets_bucket.list_blobs(prefix=f\"{name}/\"))\n",
    "        return len(blobs) > 0\n",
    "\n",
    "# Initialize dataset manager\n",
    "dataset_manager = GCSDatasetManager(client, DATASETS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Stream Datasets to GCS (All 18 Sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_columns = ['instruction', 'input', 'output']  # Standard for alignment\n",
    "\n",
    "def load_and_stream_datasets():\n",
    "    \"\"\"Load all datasets and stream them to GCS\"\"\"\n",
    "    \n",
    "    # Check if merged dataset already exists in GCS\n",
    "    if dataset_manager.dataset_exists_in_gcs('merged_uncensored_alpaca'):\n",
    "        print(\"📥 Loading pre-merged Alpaca dataset from GCS...\")\n",
    "        alpaca_merged = dataset_manager.load_dataset_from_gcs('merged_uncensored_alpaca')\n",
    "    else:\n",
    "        print(\"📤 Creating and uploading Alpaca dataset to GCS...\")\n",
    "        \n",
    "        # Exact Step 1: Alpaca uncensored merge (your code verbatim)\n",
    "        dataset_paths = [\n",
    "            \"V3N0M/Jenna-50K-Alpaca-Uncensored\",\n",
    "            \"SaisExperiments/Alpaca-Uncensored\",\n",
    "            \"SaisExperiments/Big-Alpaca-Uncensored\",\n",
    "            \"xzuyn/open-instruct-uncensored-alpaca\",\n",
    "            \"xzuyn/tulu-uncensored-alpaca\",\n",
    "            \"xzuyn/tv-alpaca-open-instruct-uncensored-blend\",\n",
    "            \"dim/dolphin_flan1m_alpaca_uncensored_3k\",\n",
    "            \"dataautogpt3/flan1m-alpaca-uncensored\",\n",
    "            \"ShubhVenom/Uncensored-Alpaca-v01\",\n",
    "            \"V3N0M/Uncensored-Alpaca\",\n",
    "            \"Xennon-BD/Alpaca-uncensored\",\n",
    "            \"VinyVan/flanMini-alpaca-uncensored_bambara\"\n",
    "        ]\n",
    "\n",
    "        # Load the first dataset to get reference columns\n",
    "        dataset1 = load_dataset(dataset_paths[0], split=\"train\")\n",
    "        reference_columns = dataset1.column_names  # Dynamic from first\n",
    "\n",
    "        # Load and select columns for the remaining datasets\n",
    "        datasets = [dataset1]\n",
    "        for path in tqdm(dataset_paths[1:], desc=\"Alpaca Loading\"):\n",
    "            dataset = load_dataset(path, split=\"train\")\n",
    "            dataset = dataset.select_columns(reference_columns)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "        # Merge all datasets\n",
    "        alpaca_merged = concatenate_datasets(datasets)\n",
    "        print(f\"Alpaca lines: {len(alpaca_merged)}\")\n",
    "        \n",
    "        # Stream to GCS\n",
    "        dataset_manager.save_dataset_to_gcs(alpaca_merged, 'merged_uncensored_alpaca')\n",
    "\n",
    "    # Load additional datasets and stream to GCS\n",
    "    datasets_to_load = [\n",
    "        (\"mrcuddle/airoboros-uncensored\", \"airoboros_uncensored\"),\n",
    "        (\"mrcuddle/airoboros-uncensored-conversation\", \"airoboros_conversation\"),\n",
    "        (\"open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-CodeLlama-34b\", \"leaderboard_winogrande\", \"harness_winogrande_5\"),\n",
    "        (\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \"leaderboard_date_understanding\", \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_date_understanding\"),\n",
    "        (\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \"leaderboard_causal_judgement\", \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_causal_judgement\"),\n",
    "        (\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \"leaderboard_boolean_expressions\", \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_boolean_expressions\")\n",
    "    ]\n",
    "    \n",
    "    loaded_datasets = [alpaca_merged]\n",
    "    \n",
    "    for dataset_info in datasets_to_load:\n",
    "        if len(dataset_info) == 2:\n",
    "            repo_name, gcs_name = dataset_info\n",
    "            subset_name = None\n",
    "        else:\n",
    "            repo_name, gcs_name, subset_name = dataset_info\n",
    "        \n",
    "        if dataset_manager.dataset_exists_in_gcs(gcs_name):\n",
    "            print(f\"📥 Loading {gcs_name} from GCS...\")\n",
    "            dataset = dataset_manager.load_dataset_from_gcs(gcs_name)\n",
    "        else:\n",
    "            print(f\"📤 Loading and uploading {gcs_name} to GCS...\")\n",
    "            \n",
    "            if subset_name:\n",
    "                dataset = load_dataset(repo_name, subset_name, split=\"train\")\n",
    "            else:\n",
    "                dataset = load_dataset(repo_name, split=\"train\")\n",
    "            \n",
    "            # Process dataset based on type\n",
    "            if 'airoboros' in gcs_name:\n",
    "                if 'conversation' in gcs_name:\n",
    "                    # Flatten conversations\n",
    "                    if 'conversations' in dataset.column_names:\n",
    "                        def flatten(ex):\n",
    "                            instr = ' '.join([t['value'] for t in ex['conversations'] if t['from'] == 'human'])\n",
    "                            out = ' '.join([t['value'] for t in ex['conversations'] if t['from'] == 'gpt'])\n",
    "                            return {'instruction': instr, 'input': '', 'output': out}\n",
    "                        dataset = dataset.map(flatten)\n",
    "                else:\n",
    "                    # Rename columns for airoboros uncensored\n",
    "                    if 'prompt' in dataset.column_names and 'instruction' not in dataset.column_names:\n",
    "                        dataset = dataset.rename_column('prompt', 'instruction')\n",
    "                    if 'completion' in dataset.column_names and 'output' not in dataset.column_names:\n",
    "                        dataset = dataset.rename_column('completion', 'output')\n",
    "            \n",
    "            elif 'leaderboard' in gcs_name:\n",
    "                # Map leaderboard format\n",
    "                def map_lb(ex):\n",
    "                    return {'instruction': ex.get('question', ''), 'input': ex.get('context', ''), 'output': ex.get('answer', '')}\n",
    "                dataset = dataset.map(map_lb)\n",
    "            \n",
    "            # Align columns and filter\n",
    "            dataset = dataset.select_columns(reference_columns)\n",
    "            dataset = dataset.filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "            \n",
    "            # Stream to GCS\n",
    "            dataset_manager.save_dataset_to_gcs(dataset, gcs_name)\n",
    "        \n",
    "        loaded_datasets.append(dataset)\n",
    "        print(f\"✅ {gcs_name}: {len(dataset)} examples\")\n",
    "    \n",
    "    return loaded_datasets, reference_columns\n",
    "\n",
    "# Load all datasets\n",
    "try:\n",
    "    all_datasets, reference_columns = load_and_stream_datasets()\n",
    "    \n",
    "    # Merge all datasets\n",
    "    final_dataset = concatenate_datasets(all_datasets)\n",
    "    final_dataset = final_dataset.filter(lambda ex: ex['output'] and len(ex['output']) > 10)\n",
    "    final_dataset = final_dataset.shuffle(seed=42)\n",
    "\n",
    "    # Sample 10% (quality preserved; uncomment for full)\n",
    "    sample_size = int(len(final_dataset) * 0.1)\n",
    "    final_dataset = final_dataset.select(range(sample_size))\n",
    "    # final_dataset = final_dataset  # Full for better quality\n",
    "\n",
    "    print(f\"Final merged (all datasets): {len(final_dataset)} examples (~{len(final_dataset)/1000:.1f}k)\")\n",
    "    \n",
    "    # Stream final dataset to GCS\n",
    "    dataset_manager.save_dataset_to_gcs(final_dataset, 'final_merged_dataset')\n",
    "    \n",
    "    !df -h  # Check disk\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}. Using fallback.\")\n",
    "    final_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\")  # Small fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model and Setup Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"ICEPVP8977/Uncensored_codegemma_7b\"\n",
    "max_seq_length = 2048  # Adjust based on your needs\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # For low VRAM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded: {model_name}\")\n",
    "print(f\"VRAM usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Format examples for training\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        if input_text:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{output}\"\n",
    "        else:\n",
    "            text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Format dataset\n",
    "final_dataset = final_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset = final_dataset.select(range(int(len(final_dataset) * 0.9)))\n",
    "eval_dataset = final_dataset.select(range(int(len(final_dataset) * 0.9), len(final_dataset)))\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
    "print(f\"Sample formatted text: {train_dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model with Streaming Checkpoints to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCSModelManager:\n",
    "    def __init__(self, client, models_bucket_name):\n",
    "        self.client = client\n",
    "        self.models_bucket = client.bucket(models_bucket_name)\n",
    "    \n",
    "    def upload_model_to_gcs(self, local_path, gcs_path):\n",
    "        \"\"\"Upload model files to GCS\"\"\"\n",
    "        print(f\"📤 Uploading model to gs://{self.models_bucket.name}/{gcs_path}...\")\n",
    "        \n",
    "        for root, dirs, files in os.walk(local_path):\n",
    "            for file in files:\n",
    "                local_file_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_file_path, local_path)\n",
    "                blob_path = f\"{gcs_path}/{relative_path}\"\n",
    "                \n",
    "                blob = self.models_bucket.blob(blob_path)\n",
    "                blob.upload_from_filename(local_file_path)\n",
    "        \n",
    "        print(f\"✅ Model uploaded to gs://{self.models_bucket.name}/{gcs_path}/\")\n",
    "    \n",
    "    def download_model_from_gcs(self, gcs_path, local_path):\n",
    "        \"\"\"Download model files from GCS\"\"\"\n",
    "        print(f\"📥 Downloading model from gs://{self.models_bucket.name}/{gcs_path}...\")\n",
    "        \n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        \n",
    "        blobs = self.models_bucket.list_blobs(prefix=gcs_path)\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('/'):  # Skip directory markers\n",
    "                continue\n",
    "                \n",
    "            relative_path = blob.name[len(gcs_path):].lstrip('/')\n",
    "            local_file_path = os.path.join(local_path, relative_path)\n",
    "            \n",
    "            # Create directory if needed\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "            \n",
    "            blob.download_to_filename(local_file_path)\n",
    "        \n",
    "        print(f\"✅ Model downloaded to {local_path}\")\n",
    "\n",
    "# Initialize model manager\n",
    "model_manager = GCSModelManager(client, MODELS_BUCKET)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=100,  # Adjust based on your needs\n",
    "    learning_rate=2e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=\"/kaggle/working/fine_tuned_model\",\n",
    "    save_steps=50,  # Save checkpoints every 50 steps\n",
    "    save_total_limit=2,  # Keep only 2 checkpoints\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Upload final model to GCS\n",
    "model_manager.upload_model_to_gcs(\"/kaggle/working/fine_tuned_model\", \"fine_tuned_model\")\n",
    "\n",
    "print(\"✅ Training completed and model uploaded to GCS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert to GGUF and Stream to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in Hugging Face format\n",
    "FastLanguageModel.for_training(model)\n",
    "model.save_pretrained(\"/kaggle/working/final_model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/final_model\")\n",
    "\n",
    "# Upload HF format to GCS\n",
    "model_manager.upload_model_to_gcs(\"/kaggle/working/final_model\", \"final_model_hf\")\n",
    "\n",
    "# Convert to GGUF\n",
    "print(\"🔄 Converting to GGUF format...\")\n",
    "%cd /tmp/llama.cpp\n",
    "\n",
    "# Convert to GGUF\n",
    "!python convert_hf_to_gguf.py /kaggle/working/final_model --outfile /kaggle/working/fine_tuned_model_f16.gguf --outtype f16\n",
    "\n",
    "# Upload GGUF to GCS\n",
    "print(\"📤 Uploading GGUF model to GCS...\")\n",
    "blob = model_manager.models_bucket.blob(\"fine_tuned_model_f16.gguf\")\n",
    "blob.upload_from_filename(\"/kaggle/working/fine_tuned_model_f16.gguf\")\n",
    "\n",
    "print(f\"✅ GGUF model uploaded to gs://{MODELS_BUCKET}/fine_tuned_model_f16.gguf\")\n",
    "\n",
    "# Check file sizes\n",
    "!ls -lh /kaggle/working/fine_tuned_model_f16.gguf\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Model and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Write a Python function to calculate fibonacci numbers:\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"🤖 Model Response:\")\n",
    "    print(response)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(f\"✅ Datasets streamed to: gs://{DATASETS_BUCKET}/\")\n",
    "print(f\"✅ Models streamed to: gs://{MODELS_BUCKET}/\")\n",
    "print(f\"✅ Final GGUF model: gs://{MODELS_BUCKET}/fine_tuned_model_f16.gguf\")\n",
    "print(f\"✅ Training completed with {len(train_dataset)} examples\")\n",
    "\n",
    "# Cleanup local files to save space\n",
    "!rm -rf /kaggle/working/fine_tuned_model\n",
    "!rm -rf /kaggle/working/final_model\n",
    "!rm -rf /kaggle/working/merged_dataset\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Check AI Compute Quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Vertex AI compute quota\n",
    "try:\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    # Initialize AI Platform\n",
    "    aiplatform.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "    \n",
    "    print(\"🔍 Checking Vertex AI compute quotas...\")\n",
    "    \n",
    "    # Note: Quota checking requires specific permissions\n",
    "    # This is a placeholder - actual quota checking would require\n",
    "    # the Cloud Resource Manager API or specific quota APIs\n",
    "    \n",
    "    print(\"📋 To check your AI compute quota:\")\n",
    "    print(\"1. Go to Google Cloud Console\")\n",
    "    print(\"2. Navigate to IAM & Admin > Quotas\")\n",
    "    print(\"3. Filter by 'Vertex AI' or 'AI Platform'\")\n",
    "    print(\"4. Check your current usage vs limits\")\n",
    "    \n",
    "    print(\"\\n💡 Common quotas to check:\")\n",
    "    print(\"- Vertex AI Training GPU hours\")\n",
    "    print(\"- Vertex AI Prediction requests\")\n",
    "    print(\"- Compute Engine GPU instances\")\n",
    "    print(\"- Cloud Storage operations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not check quotas: {e}\")\n",
    "    print(\"Please check quotas manually in Google Cloud Console\")\n",
    "\n",
    "# Alternative: Check current resource usage\n",
    "print(\"\\n📊 Current Resource Usage:\")\n",
    "!nvidia-smi\n",
    "!df -h\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Download Instructions for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📥 Instructions for downloading models in Kaggle:\")\n",
    "print(\"\\n1. Install gcloud CLI in Kaggle:\")\n",
    "print(\"   !pip install google-cloud-storage\")\n",
    "print(\"   !pip install gcsfs\")\n",
    "\n",
    "print(\"\\n2. Authenticate (use service account key):\")\n",
    "print(\"   import os\")\n",
    "print(\"   os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/your-key/key.json'\")\n",
    "\n",
    "print(\"\\n3. Download model:\")\n",
    "print(f\"   from google.cloud import storage\")\n",
    "print(f\"   client = storage.Client()\")\n",
    "print(f\"   bucket = client.bucket('{MODELS_BUCKET}')\")\n",
    "print(f\"   blob = bucket.blob('fine_tuned_model_f16.gguf')\")\n",
    "print(f\"   blob.download_to_filename('/kaggle/working/model.gguf')\")\n",
    "\n",
    "print(\"\\n4. Download datasets:\")\n",
    "print(f\"   # Use the GCSDatasetManager class from this notebook\")\n",
    "print(f\"   # to download datasets from gs://{DATASETS_BUCKET}/\")\n",
    "\n",
    "print(\"\\n✅ All models and datasets are now available in your GCS buckets!\")\n",
    "print(f\"📦 Models: gs://{MODELS_BUCKET}/\")\n",
    "print(f\"📊 Datasets: gs://{DATASETS_BUCKET}/\")\n",
    "\n",
    "# Create a simple download script for Kaggle\n",
    "download_script = f\"\"\"\n",
    "# Kaggle Download Script\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Set up authentication\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/your-key/key.json'\n",
    "\n",
    "# Initialize client\n",
    "client = storage.Client()\n",
    "models_bucket = client.bucket('{MODELS_BUCKET}')\n",
    "datasets_bucket = client.bucket('{DATASETS_BUCKET}')\n",
    "\n",
    "# Download GGUF model\n",
    "blob = models_bucket.blob('fine_tuned_model_f16.gguf')\n",
    "blob.download_to_filename('/kaggle/working/fine_tuned_model_f16.gguf')\n",
    "print('✅ Model downloaded')\n",
    "\n",
    "# Download datasets (example)\n",
    "blob = datasets_bucket.blob('final_merged_dataset/...')\n",
    "# Use GCSDatasetManager for full dataset download\n",
    "print('✅ Ready for fine-tuning in Kaggle!')\n",
    "\"\"\"\n",
    "\n",
    "with open('/kaggle/working/kaggle_download_script.py', 'w') as f:\n",
    "    f.write(download_script)\n",
    "\n",
    "print(\"\\n📄 Created kaggle_download_script.py for easy Kaggle setup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}