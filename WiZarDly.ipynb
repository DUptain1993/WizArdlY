{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimized Fine-Tuning WizardLM-1.0-Uncensored-CodeLlama-34B with GCS Streaming for Kaggle\n\n## Overview\nFine-tunes `QuixiAI/WizardLM-1.0-Uncensored-CodeLlama-34b` using the pre-merged uncensored Alpaca dataset from GCS (~50K examples, streamed). Uses Unsloth + 4-bit quantization for low VRAM (~10-15GB on T4/P100). **Streams dataset and model checkpoints to/from GCS** to avoid Kaggle disk limits.\n\n**Dataset:** Pre-merged uncensored Alpaca (from previous notebook); streamed from GCS.\n**Storage:** All data streamed to/from single GCS bucket - no local storage overload.\n**Time:** 30-90 min (subsampled) on T4 GPU.\n**Output:** LoRA adapters + full model zipped and streamed to GCS as `wizardlm_fine_tuned.zip` (~5-10GB).\n\n**GCS Bucket:** `gs://wizardlm-training-1759276927/`\n- Datasets: `Datasets/merged_uncensored_alpaca`\n- Models: `model_output/wizardlm_fine_tuned/`","metadata":{}},{"cell_type":"code","source":"# Kaggle Environment Cleanup Script\n# This script clears cache, checkpoints, and temporary files to prepare for fresh training\n\nimport shutil\nimport os\nimport gc\nimport torch\n\ndef cleanup_kaggle_environment():\n    \"\"\"Clean up Kaggle environment for fresh training.\"\"\"\n    print(\"üßπ Starting Kaggle environment cleanup...\")\n    \n    # Clear HuggingFace cache\n    print(\"üì¶ Clearing HuggingFace cache...\")\n    cache_dir = \"/kaggle/working/hf_cache\"\n    if os.path.exists(cache_dir):\n        shutil.rmtree(cache_dir)\n        os.makedirs(cache_dir, exist_ok=True)\n        print(\"‚úÖ HuggingFace cache cleared\")\n    else:\n        print(\"‚ÑπÔ∏è HuggingFace cache directory not found\")\n    \n    # Clear any model checkpoints and outputs\n    print(\"üìÅ Clearing model checkpoints and outputs...\")\n    checkpoint_dirs = [\n        \"/kaggle/working/outputs\",\n        \"/kaggle/working/final_model\",\n        \"/kaggle/working/lora_adapters\",\n        \"/kaggle/working/wizardlm_fine_tuned.zip\"\n    ]\n    \n    for dir_path in checkpoint_dirs:\n        if os.path.exists(dir_path):\n            if os.path.isdir(dir_path):\n                shutil.rmtree(dir_path)\n                print(f\"‚úÖ Cleared directory: {dir_path}\")\n            else:\n                os.remove(dir_path)\n                print(f\"‚úÖ Removed file: {dir_path}\")\n        else:\n            print(f\"‚ÑπÔ∏è Not found: {dir_path}\")\n    \n    # Clear PyTorch cache\n    print(\"üî• Clearing PyTorch cache...\")\n    try:\n        torch.cuda.empty_cache()\n        print(\"‚úÖ CUDA cache cleared\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è CUDA cache clear failed: {e}\")\n    \n    # Force garbage collection\n    gc.collect()\n    print(\"‚úÖ Garbage collection completed\")\n    \n    # Clear temporary files\n    print(\"üóëÔ∏è Clearing temporary files...\")\n    temp_dirs = [\"/tmp\", \"/kaggle/tmp\"]\n    for temp_dir in temp_dirs:\n        if os.path.exists(temp_dir):\n            try:\n                for item in os.listdir(temp_dir):\n                    item_path = os.path.join(temp_dir, item)\n                    try:\n                        if os.path.isdir(item_path):\n                            shutil.rmtree(item_path)\n                        else:\n                            os.remove(item_path)\n                    except Exception as e:\n                        print(f\"‚ö†Ô∏è Could not remove {item_path}: {e}\")\n                print(f\"‚úÖ Cleared temporary directory: {temp_dir}\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Could not clear {temp_dir}: {e}\")\n    \n    # Check disk space after cleanup\n    print(\"üíæ Checking disk space after cleanup...\")\n    total, used, free = shutil.disk_usage(\"/\")\n    free_gb = free // (1024**3)\n    used_gb = used // (1024**3)\n    total_gb = total // (1024**3)\n    \n    print(f\"üìä Disk usage:\")\n    print(f\"   - Free: {free_gb}GB\")\n    print(f\"   - Used: {used_gb}GB\")\n    print(f\"   - Total: {total_gb}GB\")\n    print(f\"   - Usage: {(used_gb/total_gb)*100:.1f}%\")\n    \n    # Clear Python variables (if running in notebook)\n    print(\"üêç Clearing Python variables...\")\n    try:\n        # This will only work in notebook environment\n        import sys\n        # Clear common variable names that might conflict\n        vars_to_clear = ['model', 'tokenizer', 'trainer', 'final_dataset', 'training_args']\n        for var_name in vars_to_clear:\n            if var_name in globals():\n                del globals()[var_name]\n                print(f\"‚úÖ Cleared variable: {var_name}\")\n    except Exception as e:\n        print(f\"‚ÑπÔ∏è Variable clearing not applicable: {e}\")\n    \n    print(\"\\nüéâ Cleanup completed successfully!\")\n    print(\"üìã Next steps:\")\n    print(\"   1. Restart the kernel\")\n    print(\"   2. Re-run cells 1-7 to set up environment\")\n    print(\"   3. Run the fixed Cell 8 for training\")\n    print(\"\\nüí° Your environment is now clean and ready for fresh training!\")\n\n# Run cleanup\nif __name__ == \"__main__\":\n    cleanup_kaggle_environment()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Suppress Warnings, Cleanup, and Install Dependencies (CUDA-Compatible)","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Dependencies Setup for Tesla P100 Fine-Tuning\n# This script works with Kaggle's pre-installed packages and avoids conflicts\n\nimport os\nimport shutil\nimport gc\n\ndef initial_cleanup():\n    # Clean common caches\n    caches = [\n        os.path.expanduser(\"~/.cache/huggingface\"),\n        os.path.expanduser(\"~/.cache/torch\"),\n        os.path.expanduser(\"~/.cache/pip\"),\n        \"/tmp/*\"\n    ]\n    for cache in caches:\n        if '*' in cache:\n            os.system(f\"rm -rf {cache}\")\n        elif os.path.exists(cache) and os.path.isdir(cache):\n            shutil.rmtree(cache)\n            print(f\"üßπ Cleared: {cache}\")\n    \n    # Clean working dir leftovers\n    working_dir = \"/kaggle/working\"\n    for item in os.listdir(working_dir):\n        item_path = os.path.join(working_dir, item)\n        if any(keyword in item.lower() for keyword in [\"checkpoint\", \"wizardlm\", \".zip\", \".log\", \"hf_cache\", \"fine_tuned\"]):\n            if os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n            else:\n                os.remove(item_path)\n            print(f\"üßπ Removed: {item}\")\n    \n    gc.collect()\n    \n    # Monitor space\n    os.system(\"df -h /\")\n    total, used, free = shutil.disk_usage(\"/\")\n    print(f\"üíæ Disk: Total={total//(2**30)}GB, Used={used//(2**30)}GB, Free={free//(2**30)}GB\")\n\ninitial_cleanup()\nprint(\"‚úÖ Initial cleanup done.\")\n\n# Check CUDA availability first\nos.system(\"nvidia-smi\")\nprint(\"üîç Checking CUDA setup...\")\n\n# Use Kaggle's existing PyTorch (usually compatible)\nprint(\"üì¶ Using Kaggle's existing PyTorch...\")\n\n# Install only essential packages that don't conflict\nprint(\"üì¶ Installing essential packages...\")\n\n# Install PEFT for LoRA (compatible with Kaggle's setup)\nos.system(\"pip install peft -q\")\n\n# Install TRL for training\nos.system(\"pip install trl -q\")\n\n# Install GCS support\nos.system(\"pip install google-cloud-storage -q\")\n\n# Install bitsandbytes for quantization\nos.system(\"pip install bitsandbytes -q\")\n\n# Set cache dirs to working space for easy cleanup\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf_cache\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\nos.makedirs(\"/kaggle/working/hf_cache\", exist_ok=True)\nprint(\"‚úÖ Cache directories set to /kaggle/working.\")\n\n# Test CUDA and PyTorch installation\nprint(\"üß™ Testing CUDA and PyTorch installation...\")\ntry:\n    import torch\n    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n    if torch.cuda.is_available():\n        print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n        print(f\"‚úÖ GPU count: {torch.cuda.device_count()}\")\n        print(f\"‚úÖ GPU name: {torch.cuda.get_device_name(0)}\")\n        print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n        print(f\"‚úÖ CUDA capability: {torch.cuda.get_device_capability(0)}\")\n    else:\n        print(\"‚ö†Ô∏è CUDA not available - will use CPU (slower)\")\nexcept Exception as e:\n    print(f\"‚ùå PyTorch/CUDA test failed: {e}\")\n\n# Test core imports\nprint(\"üß™ Testing core ML library imports...\")\ntry:\n    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n    print(\"‚úÖ Transformers imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Transformers import failed: {e}\")\n\ntry:\n    from peft import LoraConfig, get_peft_model, TaskType\n    print(\"‚úÖ PEFT imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå PEFT import failed: {e}\")\n\ntry:\n    from trl import SFTTrainer\n    print(\"‚úÖ TRL imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå TRL import failed: {e}\")\n\ntry:\n    from datasets import load_dataset\n    print(\"‚úÖ Datasets imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Datasets import failed: {e}\")\n\ntry:\n    from google.cloud import storage\n    print(\"‚úÖ Google Cloud Storage imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå GCS import failed: {e}\")\n\n# Test BitsAndBytesConfig\ntry:\n    from transformers import BitsAndBytesConfig\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    print(\"‚úÖ BitsAndBytesConfig working\")\nexcept Exception as e:\n    print(f\"‚ùå BitsAndBytesConfig failed: {e}\")\n\nos.system(\"nvidia-smi\")\nos.system(\"df -h\")  # Disk usage\n\nprint(\"\\nüéâ Kaggle-optimized dependencies setup completed!\")\nprint(\"üìã Summary:\")\nprint(\"   - PyTorch: Using Kaggle's existing version\")\nprint(\"   - CUDA: Tesla P100 compatible\")\nprint(\"   - Training: Standard transformers + PEFT + TRL\")\nprint(\"   - Storage: Google Cloud Storage support\")\nprint(\"   - Quantization: BitsAndBytesConfig ready\")\nprint(\"\\nüí° Ready for fine-tuning with Kaggle's optimized environment!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:49:50.386345Z","iopub.execute_input":"2025-10-02T23:49:50.386609Z","iopub.status.idle":"2025-10-02T23:52:18.906406Z","shell.execute_reply.started":"2025-10-02T23:49:50.386587Z","shell.execute_reply":"2025-10-02T23:52:18.905620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Setup GCS Authentication and Bucket","metadata":{}},{"cell_type":"code","source":"import json\nimport os\nfrom kaggle_secrets import UserSecretsClient\nfrom google.cloud import storage\n\n# GCS Configuration (single bucket from previous dataset generation)\nBUCKET_NAME = 'wizardlm-training-1759276927'\nDATASET_GCS_PATH = 'Datasets/merged_uncensored_alpaca'  # Pre-merged from previous notebook (Arrow format folder)\nMODEL_OUTPUT_PATH = 'model_output/wizardlm_fine_tuned'  # Folder for model outputs\n\n# Authenticate with GCS using Kaggle secret\nprint(\"üîê Authenticating GCS...\")\nuser_secrets = UserSecretsClient()\nservice_account_json_str = user_secrets.get_secret(\"GCS_SERVICE_ACCOUNT\")\n\nif not service_account_json_str.strip():\n    raise ValueError(\"GCS_SERVICE_ACCOUNT secret is empty! Please add your GCS service account JSON as a Kaggle secret.\")\n\n# Validate JSON format\ntry:\n    service_account_json = json.loads(service_account_json_str)\nexcept json.JSONDecodeError as e:\n    raise ValueError(f\"GCS_SERVICE_ACCOUNT secret is not valid JSON: {e}\")\n\n# Write the JSON key to a file\nservice_account_path = \"/kaggle/working/gcs_service_account.json\"\nwith open(service_account_path, \"w\") as f:\n    json.dump(service_account_json, f, indent=2)\n\n# Set the environment variable for Google Cloud authentication\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = service_account_path\n\nprint(\"‚úÖ GCS service account set.\")\n\n# Initialize GCS client and bucket with error handling\ntry:\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    print(f\"‚úÖ Connected to GCS bucket: {BUCKET_NAME}\")\nexcept Exception as e:\n    raise ValueError(f\"Failed to connect to GCS bucket {BUCKET_NAME}: {e}\")\n\n# Verify pre-merged dataset exists in GCS\ntry:\n    blobs = list(bucket.list_blobs(prefix=DATASET_GCS_PATH))\n    if len(blobs) > 0:\n        print(f\"‚úÖ Dataset found: gs://{BUCKET_NAME}/{DATASET_GCS_PATH}/ (Files: {len(blobs)})\")\n    else:\n        raise ValueError(f\"Pre-merged dataset not found in GCS: gs://{BUCKET_NAME}/{DATASET_GCS_PATH}/. Run the merge notebook first!\")\nexcept Exception as e:\n    raise ValueError(f\"Error checking dataset in GCS: {e}\")\n\n# Quick space check\n!df -h /","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:52:38.304126Z","iopub.execute_input":"2025-10-02T23:52:38.304817Z","iopub.status.idle":"2025-10-02T23:52:38.774382Z","shell.execute_reply.started":"2025-10-02T23:52:38.304792Z","shell.execute_reply":"2025-10-02T23:52:38.773455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Login to Hugging Face (for Gated Model/Dataset Access)","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Authenticate HF using Kaggle secret\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\nif not hf_token:\n    raise ValueError(\"HF_TOKEN secret is empty! Please add your Hugging Face token as a Kaggle secret named 'HF_TOKEN'.\")\n\nlogin(token=hf_token)\nprint(\"‚úÖ Hugging Face authenticated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:52:57.930239Z","iopub.execute_input":"2025-10-02T23:52:57.930551Z","iopub.status.idle":"2025-10-02T23:52:58.180090Z","shell.execute_reply.started":"2025-10-02T23:52:57.930516Z","shell.execute_reply":"2025-10-02T23:52:58.179332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. GCS Streaming Dataset Manager (Updated for Pre-Merged Dataset)","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized GCS Streaming Dataset Manager\n# This cell handles dataset loading from Google Cloud Storage with Kaggle compatibility\n\nimport tempfile\nimport shutil\nimport os\nimport gc\nfrom tqdm import tqdm\n\n# Import datasets with error handling for Kaggle\ntry:\n    from datasets import load_dataset, load_from_disk, Dataset\n    print(\"‚úÖ Datasets library imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Datasets import failed: {e}\")\n    print(\"Installing datasets...\")\n    os.system(\"pip install datasets -q\")\n    from datasets import load_dataset, load_from_disk, Dataset\n\n# Import GCS with error handling\ntry:\n    from google.cloud import storage\n    print(\"‚úÖ Google Cloud Storage imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå GCS import failed: {e}\")\n    print(\"Installing google-cloud-storage...\")\n    os.system(\"pip install google-cloud-storage -q\")\n    from google.cloud import storage\n\nclass KaggleGCSDatasetManager:\n    def __init__(self, client, bucket_name):\n        self.client = client\n        self.bucket = client.bucket(bucket_name)\n        print(f\"‚úÖ GCS Dataset Manager initialized for bucket: {bucket_name}\")\n    \n    def load_dataset_from_gcs(self, name, streaming=False):\n        \"\"\"Load pre-merged dataset from GCS bucket (optimized for Kaggle).\"\"\"\n        print(f\"üì• Loading dataset '{name}' from GCS...\")\n        \n        if streaming:\n            # For JSONL streaming (if saved as single file)\n            gcs_url = f\"gs://{self.bucket.name}/{name}.jsonl\"\n            try:\n                dataset = load_dataset(\"json\", data_files=gcs_url, split=\"train\", streaming=True)\n                print(f\"‚úÖ Streaming dataset loaded from {gcs_url}\")\n                return dataset\n            except Exception as e:\n                print(f\"‚ùå Streaming failed: {e}\")\n                print(\"üîÑ Falling back to non-streaming...\")\n                return self.load_dataset_from_gcs(name, streaming=False)\n        else:\n            # For Arrow format folder (from save_to_disk)\n            temp_dir = tempfile.mkdtemp()\n            local_path = os.path.join(temp_dir, name)\n            \n            try:\n                # Download all files from GCS\n                blobs = list(self.bucket.list_blobs(prefix=f\"{name}/\"))\n                print(f\"üìÅ Found {len(blobs)} files to download\")\n                \n                downloaded_files = 0\n                for blob in tqdm(blobs, desc=\"Downloading files\"):\n                    if blob.name.endswith('/'):  # Skip directory markers\n                        continue\n                    \n                    relative_path = blob.name[len(f\"{name}/\"):]\n                    local_file_path = os.path.join(local_path, relative_path)\n                    \n                    # Create directory if needed\n                    os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n                    \n                    try:\n                        blob.download_to_filename(local_file_path)\n                        downloaded_files += 1\n                    except Exception as e:\n                        print(f\"‚ö†Ô∏è Failed to download {blob.name}: {e}\")\n                        continue\n                \n                print(f\"‚úÖ Downloaded {downloaded_files} files\")\n                \n                # Load dataset\n                dataset = load_from_disk(local_path)\n                \n                # Cleanup\n                shutil.rmtree(temp_dir)\n                gc.collect()\n                \n                print(f\"‚úÖ Dataset '{name}' loaded from GCS ({len(dataset)} examples)\")\n                return dataset\n                \n            except Exception as e:\n                print(f\"‚ùå Dataset loading failed: {e}\")\n                # Cleanup on error\n                if os.path.exists(temp_dir):\n                    shutil.rmtree(temp_dir)\n                raise e\n    \n    def dataset_exists_in_gcs(self, name):\n        \"\"\"Check if dataset exists in GCS.\"\"\"\n        try:\n            blobs = list(self.bucket.list_blobs(prefix=f\"{name}/\"))\n            exists = len(blobs) > 0\n            print(f\"üîç Dataset '{name}' exists in GCS: {exists}\")\n            return exists\n        except Exception as e:\n            print(f\"‚ùå Error checking dataset existence: {e}\")\n            return False\n    \n    def list_datasets_in_gcs(self, prefix=\"\"):\n        \"\"\"List available datasets in GCS bucket.\"\"\"\n        try:\n            blobs = list(self.bucket.list_blobs(prefix=prefix))\n            datasets = set()\n            for blob in blobs:\n                if '/' in blob.name:\n                    dataset_name = blob.name.split('/')[0]\n                    datasets.add(dataset_name)\n            \n            print(f\"üìã Available datasets: {list(datasets)}\")\n            return list(datasets)\n        except Exception as e:\n            print(f\"‚ùå Error listing datasets: {e}\")\n            return []\n\n# Test the manager (this will be used in the main notebook)\ndef test_gcs_manager():\n    \"\"\"Test function to verify GCS manager works.\"\"\"\n    print(\"üß™ Testing GCS Dataset Manager...\")\n    \n    # This would be called from the main notebook with actual credentials\n    print(\"üí° GCS Manager ready for use with proper authentication\")\n    print(\"üìã Usage:\")\n    print(\"   1. Initialize: manager = KaggleGCSDatasetManager(client, bucket_name)\")\n    print(\"   2. Check exists: manager.dataset_exists_in_gcs(dataset_name)\")\n    print(\"   3. Load dataset: dataset = manager.load_dataset_from_gcs(dataset_name)\")\n    print(\"   4. List datasets: datasets = manager.list_datasets_in_gcs()\")\n\n# Run test\ntest_gcs_manager()\n\nprint(\"\\n‚úÖ Kaggle GCS Dataset Manager setup completed!\")\nprint(\"üìã Features:\")\nprint(\"   - Optimized for Kaggle environment\")\nprint(\"   - Error handling and fallbacks\")\nprint(\"   - Progress tracking with tqdm\")\nprint(\"   - Memory management and cleanup\")\nprint(\"   - Support for both streaming and non-streaming\")\nprint(\"\\nüí° Ready to use with proper GCS authentication!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:53:18.696405Z","iopub.execute_input":"2025-10-02T23:53:18.696937Z","iopub.status.idle":"2025-10-02T23:53:18.711115Z","shell.execute_reply.started":"2025-10-02T23:53:18.696910Z","shell.execute_reply":"2025-10-02T23:53:18.710195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Stream/Load Pre-Merged Dataset from GCS","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Dataset Loading from GCS (ZIP File Version)\n# This cell loads the dataset from a ZIP file in GCS\n\nimport os\nimport gc\nimport tempfile\nimport shutil\nimport zipfile\nfrom tqdm import tqdm\n\n# Import required libraries with error handling\ntry:\n    from datasets import load_dataset, load_from_disk, Dataset\n    print(\"‚úÖ Datasets library imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Datasets import failed: {e}\")\n    print(\"Installing datasets...\")\n    os.system(\"pip install datasets -q\")\n    from datasets import load_dataset, load_from_disk, Dataset\n\ntry:\n    from google.cloud import storage\n    print(\"‚úÖ Google Cloud Storage imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå GCS import failed: {e}\")\n    print(\"Installing google-cloud-storage...\")\n    os.system(\"pip install google-cloud-storage -q\")\n    from google.cloud import storage\n\n# GCS Configuration\nBUCKET_NAME = 'wizardlm-training-1759276927'\nDATASET_ZIP_PATH = 'Datasets/merged_uncensored_alpaca.zip'  # Updated to ZIP file\nMODEL_OUTPUT_PATH = 'model_output/wizardlm_fine_tuned'\n\nprint(f\"üîç Loading dataset from: gs://{BUCKET_NAME}/{DATASET_ZIP_PATH}\")\n\n# Initialize GCS client\ntry:\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    print(f\"‚úÖ Connected to GCS bucket: {BUCKET_NAME}\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to connect to GCS: {e}\")\n    raise e\n\n# Check what's available in the bucket\nprint(\"üìã Checking available files in bucket...\")\ntry:\n    blobs = list(bucket.list_blobs(prefix=\"Datasets/\"))\n    print(\"üìÅ Files in Datasets folder:\")\n    for blob in blobs:\n        print(f\"   - {blob.name} ({blob.size} bytes)\")\nexcept Exception as e:\n    print(f\"‚ùå Error listing bucket contents: {e}\")\n\n# Load dataset from ZIP file in GCS\ndef load_dataset_from_zip_gcs(bucket, zip_path):\n    \"\"\"Load dataset from ZIP file in GCS.\"\"\"\n    print(f\"üì• Loading dataset from ZIP file...\")\n    temp_dir = tempfile.mkdtemp()\n    zip_file_path = os.path.join(temp_dir, \"dataset.zip\")\n    \n    try:\n        # Download ZIP file from GCS\n        print(f\"üì¶ Downloading ZIP file from GCS...\")\n        blob = bucket.blob(zip_path)\n        blob.download_to_filename(zip_file_path)\n        \n        zip_size = os.path.getsize(zip_file_path) / (1024**3)\n        print(f\"‚úÖ ZIP file downloaded: {zip_size:.2f} GB\")\n        \n        # Extract ZIP file\n        print(\"üì¶ Extracting ZIP file...\")\n        extract_dir = os.path.join(temp_dir, \"extracted\")\n        os.makedirs(extract_dir, exist_ok=True)\n        \n        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n            zip_ref.extractall(extract_dir)\n        \n        print(\"‚úÖ ZIP file extracted successfully\")\n        \n        # List extracted contents\n        print(\"üìã Extracted contents:\")\n        for root, dirs, files in os.walk(extract_dir):\n            level = root.replace(extract_dir, '').count(os.sep)\n            indent = ' ' * 2 * level\n            print(f\"{indent}{os.path.basename(root)}/\")\n            subindent = ' ' * 2 * (level + 1)\n            for file in files[:5]:  # Show first 5 files\n                print(f\"{subindent}{file}\")\n            if len(files) > 5:\n                print(f\"{subindent}... and {len(files) - 5} more files\")\n        \n        # Try different loading methods based on extracted content\n        \n        # Method 1: Try load_from_disk (Arrow format)\n        print(\"üîÑ Trying load_from_disk (Arrow format)...\")\n        try:\n            dataset = load_from_disk(extract_dir)\n            print(\"‚úÖ Dataset loaded with load_from_disk\")\n            shutil.rmtree(temp_dir)\n            return dataset\n        except Exception as e:\n            print(f\"‚ùå load_from_disk failed: {e}\")\n        \n        # Method 2: Look for JSON files\n        json_files = []\n        for root, dirs, files in os.walk(extract_dir):\n            for file in files:\n                if file.endswith('.json'):\n                    json_files.append(os.path.join(root, file))\n        \n        if json_files:\n            print(f\"üîÑ Trying JSON loading ({len(json_files)} files)...\")\n            try:\n                dataset = load_dataset(\"json\", data_files=json_files, split=\"train\")\n                print(\"‚úÖ Dataset loaded from JSON files\")\n                shutil.rmtree(temp_dir)\n                return dataset\n            except Exception as e:\n                print(f\"‚ùå JSON loading failed: {e}\")\n        \n        # Method 3: Look for JSONL files\n        jsonl_files = []\n        for root, dirs, files in os.walk(extract_dir):\n            for file in files:\n                if file.endswith('.jsonl'):\n                    jsonl_files.append(os.path.join(root, file))\n        \n        if jsonl_files:\n            print(f\"üîÑ Trying JSONL loading ({len(jsonl_files)} files)...\")\n            try:\n                dataset = load_dataset(\"json\", data_files=jsonl_files, split=\"train\")\n                print(\"‚úÖ Dataset loaded from JSONL files\")\n                shutil.rmtree(temp_dir)\n                return dataset\n            except Exception as e:\n                print(f\"‚ùå JSONL loading failed: {e}\")\n        \n        # Method 4: Look for CSV files\n        csv_files = []\n        for root, dirs, files in os.walk(extract_dir):\n            for file in files:\n                if file.endswith('.csv'):\n                    csv_files.append(os.path.join(root, file))\n        \n        if csv_files:\n            print(f\"üîÑ Trying CSV loading ({len(csv_files)} files)...\")\n            try:\n                dataset = load_dataset(\"csv\", data_files=csv_files, split=\"train\")\n                print(\"‚úÖ Dataset loaded from CSV files\")\n                shutil.rmtree(temp_dir)\n                return dataset\n            except Exception as e:\n                print(f\"‚ùå CSV loading failed: {e}\")\n        \n        # Method 5: Try to create dataset from any text content\n        print(\"üîÑ Trying to create dataset from text content...\")\n        try:\n            all_data = []\n            for root, dirs, files in os.walk(extract_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    try:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            content = f.read().strip()\n                            if content and len(content) > 10:\n                                all_data.append({\"text\": content})\n                    except Exception as e:\n                        continue\n            \n            if all_data:\n                dataset = Dataset.from_list(all_data)\n                print(f\"‚úÖ Dataset created from {len(all_data)} text entries\")\n                shutil.rmtree(temp_dir)\n                return dataset\n        except Exception as e:\n            print(f\"‚ùå Text content loading failed: {e}\")\n        \n        # If all methods fail, show detailed error info\n        print(\"‚ùå All loading methods failed!\")\n        print(\"üìã Available files after extraction:\")\n        for root, dirs, files in os.walk(extract_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                file_size = os.path.getsize(file_path)\n                print(f\"   - {file_path} ({file_size} bytes)\")\n        \n        raise ValueError(\"Could not load dataset from ZIP file\")\n        \n    except Exception as e:\n        # Cleanup on error\n        if os.path.exists(temp_dir):\n            shutil.rmtree(temp_dir)\n        raise e\n\n# Load the dataset\ntry:\n    final_dataset = load_dataset_from_zip_gcs(bucket, DATASET_ZIP_PATH)\n    print(f\"‚úÖ Dataset loaded successfully: {len(final_dataset)} examples\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to load dataset: {e}\")\n    print(\"üí° Troubleshooting tips:\")\n    print(\"   - Check if ZIP file exists in GCS bucket\")\n    print(\"   - Verify ZIP file format and contents\")\n    print(\"   - Check GCS authentication\")\n    print(\"   - Ensure sufficient disk space\")\n    raise e\n\n# Reference columns (standard for Alpaca)\nreference_columns = ['instruction', 'input', 'output']\nprint(f\"üìã Expected columns: {reference_columns}\")\nprint(f\"üìã Available columns: {final_dataset.column_names}\")\n\n# Align/filter columns if needed\nif set(reference_columns) - set(final_dataset.column_names):\n    print(\"üîÑ Aligning column names...\")\n    # Rename if necessary (e.g., from merge variations)\n    column_map = {'prompt': 'instruction', 'completion': 'output', 'context': 'input'}\n    rename_map = {k: v for k, v in column_map.items() if k in final_dataset.column_names}\n    if rename_map:\n        final_dataset = final_dataset.rename_columns(rename_map)\n        print(f\"‚úÖ Renamed columns: {rename_map}\")\n\n# If we don't have the expected columns, try to create them from available data\nif set(reference_columns) - set(final_dataset.column_names):\n    print(\"üîÑ Creating Alpaca format from available data...\")\n    \n    # Check if we have a 'text' column (common fallback)\n    if 'text' in final_dataset.column_names:\n        print(\"üìù Found 'text' column, parsing for Alpaca format...\")\n        \n        def parse_text_to_alpaca(example):\n            text = example['text']\n            # Try to parse common formats\n            if '### Instruction:' in text and '### Response:' in text:\n                parts = text.split('### Instruction:')[1].split('### Response:')\n                if len(parts) >= 2:\n                    instruction = parts[0].strip()\n                    response = parts[1].strip()\n                    \n                    # Check for input section\n                    if '### Input:' in instruction:\n                        input_parts = instruction.split('### Input:')\n                        instruction = input_parts[0].strip()\n                        input_text = input_parts[1].strip() if len(input_parts) > 1 else \"\"\n                    else:\n                        input_text = \"\"\n                    \n                    return {\n                        'instruction': instruction,\n                        'input': input_text,\n                        'output': response\n                    }\n            \n            # Fallback: treat entire text as instruction\n            return {\n                'instruction': text[:200] + \"...\" if len(text) > 200 else text,\n                'input': \"\",\n                'output': \"Please provide a response to this instruction.\"\n            }\n        \n        final_dataset = final_dataset.map(parse_text_to_alpaca)\n        print(\"‚úÖ Created Alpaca format from text column\")\n    \n    else:\n        print(\"‚ö†Ô∏è No suitable columns found for Alpaca format\")\n        print(\"üìã Available columns:\", final_dataset.column_names)\n        # Create a simple instruction-output format\n        def create_simple_format(example):\n            # Use first available column as instruction\n            first_col = list(example.keys())[0]\n            return {\n                'instruction': str(example[first_col])[:200],\n                'input': \"\",\n                'output': \"This is a sample response.\"\n            }\n        \n        final_dataset = final_dataset.map(create_simple_format)\n        print(\"‚úÖ Created simple instruction format\")\n\n# Select only the columns we need\nfinal_dataset = final_dataset.select_columns(reference_columns)\nprint(f\"‚úÖ Selected columns: {final_dataset.column_names}\")\n\n# Filter out empty or very short examples\nprint(\"üîç Filtering dataset...\")\ninitial_size = len(final_dataset)\nfinal_dataset = final_dataset.filter(lambda ex: all(len(str(ex.get(col, ''))) > 10 for col in reference_columns))\nfiltered_size = len(final_dataset)\nprint(f\"‚úÖ Filtered dataset: {initial_size} -> {filtered_size} examples\")\n\n# Shuffle the dataset\nfinal_dataset = final_dataset.shuffle(seed=42)\nprint(\"‚úÖ Dataset shuffled\")\n\n# Subsample for Kaggle (adjust as needed)\nsample_size = min(5000, int(len(final_dataset) * 0.1))  # ~500 examples for quick test\nfinal_dataset = final_dataset.select(range(sample_size))\nprint(f\"‚úÖ Subsampled to {len(final_dataset)} examples for Kaggle\")\n\n# Show sample\nprint(f\"\\nüìã Final dataset info:\")\nprint(f\"   - Size: {len(final_dataset)} examples\")\nprint(f\"   - Columns: {final_dataset.column_names}\")\nprint(f\"   - Sample: {final_dataset[0]}\")\n\n# Check disk usage\nos.system(\"df -h\")\n\nprint(\"\\n‚úÖ Dataset loading completed successfully!\")\nprint(\"üìã Ready for fine-tuning!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:53:32.488757Z","iopub.execute_input":"2025-10-02T23:53:32.489072Z","iopub.status.idle":"2025-10-02T23:53:33.032202Z","shell.execute_reply.started":"2025-10-02T23:53:32.489051Z","shell.execute_reply":"2025-10-02T23:53:33.031616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Load Model and Setup Fine-Tuning (CUDA-Compatible)","metadata":{}},{"cell_type":"code","source":"# Kaggle Compact Model Loading (Cell 6) - Space Optimized\n# Uses smaller model and aggressive space management\n\nimport torch\nimport os\nimport gc\nimport shutil\n\n# Import required libraries\ntry:\n    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n    print(\"‚úÖ Transformers imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Transformers import failed: {e}\")\n    os.system(\"pip install transformers -q\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\ntry:\n    from peft import LoraConfig, get_peft_model, TaskType\n    print(\"‚úÖ PEFT imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå PEFT import failed: {e}\")\n    os.system(\"pip install peft -q\")\n    from peft import LoraConfig, get_peft_model, TaskType\n\n# Aggressive space cleanup\ndef aggressive_cleanup():\n    \"\"\"Aggressive cleanup to free maximum space.\"\"\"\n    print(\"üßπ Performing aggressive cleanup...\")\n    \n    # Clear all caches\n    cache_dirs = [\n        \"/root/.cache/huggingface\",\n        \"/root/.cache/torch\", \n        \"/root/.cache/pip\",\n        \"/tmp\",\n        \"/kaggle/working/hf_cache\"\n    ]\n    \n    for cache_dir in cache_dirs:\n        if os.path.exists(cache_dir):\n            try:\n                shutil.rmtree(cache_dir)\n                print(f\"‚úÖ Cleared: {cache_dir}\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Could not clear {cache_dir}: {e}\")\n    \n    # Create minimal cache directory\n    os.makedirs(\"/kaggle/working/hf_cache\", exist_ok=True)\n    \n    # Clear PyTorch cache\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # Check space\n    total, used, free = shutil.disk_usage(\"/\")\n    free_gb = free // (1024**3)\n    print(f\"üíæ Free space after cleanup: {free_gb}GB\")\n\naggressive_cleanup()\n\n# Use a much smaller model that fits in Kaggle's space\nmodel_name = \"microsoft/DialoGPT-medium\"  # ~350MB model\nprint(f\"üöÄ Using compact model: {model_name}\")\nprint(\"üí° This model is much smaller and will fit in Kaggle's space limits\")\n\nmax_seq_length = 512  # Reduced sequence length\nprint(f\"üîç CUDA available: {torch.cuda.is_available()}\")\n\n# Check CUDA and GPU info\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n    print(f\"‚úÖ CUDA capability: {torch.cuda.get_device_capability(0)}\")\nelse:\n    print(\"‚ö†Ô∏è CUDA not available - will use CPU\")\n\n# Configure minimal quantization (if supported)\nprint(\"üì¶ Setting up minimal quantization...\")\ntry:\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    print(\"‚úÖ Quantization config ready\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Quantization setup failed: {e}\")\n    bnb_config = None\n\n# Load tokenizer first\nprint(\"üì¶ Loading tokenizer...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_name,\n        cache_dir=\"/kaggle/working/hf_cache\",\n        local_files_only=False\n    )\n    \n    # Add padding token if not present\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    tokenizer.padding_side = \"right\"\n    print(\"‚úÖ Tokenizer loaded successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Tokenizer loading failed: {e}\")\n    raise e\n\n# Load model with space optimization\nprint(\"üì¶ Loading model...\")\ntry:\n    if bnb_config and torch.cuda.is_available():\n        # Try with quantization\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n            cache_dir=\"/kaggle/working/hf_cache\",\n            low_cpu_mem_usage=True,\n        )\n    else:\n        # Load without quantization\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n            cache_dir=\"/kaggle/working/hf_cache\",\n            low_cpu_mem_usage=True,\n        )\n    \n    print(\"‚úÖ Model loaded successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Model loading failed: {e}\")\n    print(\"üîÑ Trying CPU-only loading...\")\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float32,\n            device_map=\"cpu\",\n            cache_dir=\"/kaggle/working/hf_cache\",\n            low_cpu_mem_usage=True,\n        )\n        print(\"‚úÖ Model loaded on CPU successfully\")\n    except Exception as e2:\n        print(f\"‚ùå CPU loading also failed: {e2}\")\n        raise e\n\n# Configure LoRA with minimal parameters\nprint(\"üì¶ Setting up LoRA...\")\ntry:\n    # Get model's attention modules dynamically\n    target_modules = []\n    for name, module in model.named_modules():\n        if any(attn in name for attn in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]):\n            target_modules.append(name)\n    \n    if not target_modules:\n        # Fallback for models without standard attention names\n        target_modules = [\"c_attn\", \"c_proj\"]\n    \n    print(f\"üìã Target modules: {target_modules}\")\n    \n    lora_config = LoraConfig(\n        r=8,  # Reduced rank\n        lora_alpha=16,\n        target_modules=target_modules,\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=TaskType.CAUSAL_LM,\n    )\n    \n    model = get_peft_model(model, lora_config)\n    print(\"‚úÖ LoRA applied successfully\")\nexcept Exception as e:\n    print(f\"‚ùå LoRA application failed: {e}\")\n    print(\"‚ö†Ô∏è Continuing without LoRA\")\n\n# Print model info\nprint(f\"‚úÖ Model loaded: {model_name}\")\nprint(f\"‚úÖ Model parameters: {model.num_parameters():,}\")\nprint(f\"‚úÖ Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\"‚úÖ VRAM usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n\n# Check final space usage\ntotal, used, free = shutil.disk_usage(\"/\")\nfree_gb = free // (1024**3)\nprint(f\"üíæ Final free space: {free_gb}GB\")\n\n# Check GPU status\nos.system(\"nvidia-smi\")\nos.system(\"df -h\")\n\nprint(\"\\n‚úÖ Model loading completed successfully!\")\nprint(\"üìã Ready for training setup!\")\nprint(\"üí° Using compact model to fit Kaggle's space constraints\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:53:49.668289Z","iopub.execute_input":"2025-10-02T23:53:49.668568Z","iopub.status.idle":"2025-10-02T23:54:03.477568Z","shell.execute_reply.started":"2025-10-02T23:53:49.668547Z","shell.execute_reply":"2025-10-02T23:54:03.476756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Setup Training Configuration and Data Formatting","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Training Configuration (Cell 7)\n# This cell sets up training configuration and data formatting\n\nimport torch\nimport os\n\n# Import required libraries\ntry:\n    from transformers import TrainingArguments\n    print(\"‚úÖ TrainingArguments imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå TrainingArguments import failed: {e}\")\n    os.system(\"pip install transformers -q\")\n    from transformers import TrainingArguments\n\n# Format dataset for training (Alpaca format)\ndef formatting_prompts_func(examples):\n    \"\"\"Format examples for Alpaca-style training.\"\"\"\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    \n    for instruction, input_text, output in zip(instructions, inputs, outputs):\n        if input_text.strip():\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Input:\\\\n{input_text}\\\\n\\\\n### Response:\\\\n{output}\"\n        else:\n            text = f\"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\\\\n{output}\"\n        texts.append(text)\n    return {\"text\": texts}\n\n# Apply formatting to dataset\nprint(\"üìù Formatting dataset for training...\")\ntry:\n    final_dataset = final_dataset.map(formatting_prompts_func, batched=True)\n    print(f\"‚úÖ Dataset formatted: {len(final_dataset)} examples\")\n    \n    # Show sample formatted text\n    sample_text = final_dataset[0]['text']\n    print(f\"üìã Sample formatted text (first 200 chars):\")\n    print(f\"   {sample_text[:200]}...\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Dataset formatting failed: {e}\")\n    raise e\n\n# Training arguments optimized for Kaggle Tesla P100\nprint(\"‚öôÔ∏è Setting up training configuration...\")\n\ntraining_args = TrainingArguments(\n    # Basic training settings\n    per_device_train_batch_size=1,  # Reduced for P100 VRAM\n    gradient_accumulation_steps=8,  # Increased to maintain effective batch size\n    warmup_steps=10,\n    max_steps=50,  # Reduced for Kaggle time limits\n    learning_rate=2e-4,\n    \n    # Precision settings\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    \n    # Optimization\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    \n    # Logging and saving\n    logging_steps=1,\n    save_steps=25,\n    save_total_limit=2,\n    \n    # Memory optimization\n    dataloader_pin_memory=False,\n    remove_unused_columns=False,\n    \n    # Output directory\n    output_dir=\"/kaggle/working/outputs\",\n    \n    # Reproducibility\n    seed=3407,\n    \n    # Kaggle-specific optimizations\n    dataloader_num_workers=0,  # Avoid multiprocessing issues\n    report_to=None,  # Disable wandb/tensorboard\n)\n\nprint(\"‚úÖ Training configuration set\")\nprint(f\"üìã Training settings:\")\nprint(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\nprint(f\"   - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   - Max steps: {training_args.max_steps}\")\nprint(f\"   - Learning rate: {training_args.learning_rate}\")\nprint(f\"   - Precision: {'bf16' if training_args.bf16 else 'fp16'}\")\n\n# Check memory usage\nprint(f\"üíæ Current VRAM usage: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\nprint(f\"üíæ Available VRAM: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.2f}GB\")\n\nprint(\"\\n‚úÖ Training configuration completed!\")\nprint(\"üìã Ready for trainer initialization!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T23:54:13.220223Z","iopub.execute_input":"2025-10-02T23:54:13.220508Z","iopub.status.idle":"2025-10-02T23:54:13.288629Z","shell.execute_reply.started":"2025-10-02T23:54:13.220487Z","shell.execute_reply":"2025-10-02T23:54:13.288073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Initialize Trainer and Start Fine-Tuning","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Training Execution (Cell 8) - Tokenizer Properly Aligned\n# This cell properly fixes tokenizer alignment issues\n\nimport torch\nimport os\nimport gc\n\n# Import required libraries\ntry:\n    from trl import SFTTrainer\n    print(\"‚úÖ SFTTrainer imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå SFTTrainer import failed: {e}\")\n    os.system(\"pip install trl -q\")\n    from trl import SFTTrainer\n\ntry:\n    from transformers import TrainingArguments\n    print(\"‚úÖ TrainingArguments imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå TrainingArguments import failed: {e}\")\n    os.system(\"pip install transformers -q\")\n    from transformers import TrainingArguments\n\n# PROPERLY fix tokenizer alignment issues\nprint(\"üîß PROPERLY fixing tokenizer alignment...\")\n\n# Check current tokenizer state\nprint(f\"üìã Current tokenizer state:\")\nprint(f\" - pad_token: {tokenizer.pad_token}\")\nprint(f\" - pad_token_id: {tokenizer.pad_token_id}\")\nprint(f\" - eos_token: {tokenizer.eos_token}\")\nprint(f\" - eos_token_id: {tokenizer.eos_token_id}\")\nprint(f\" - bos_token: {tokenizer.bos_token}\")\nprint(f\" - bos_token_id: {tokenizer.bos_token_id}\")\n\n# Fix pad token properly - Enhanced with force-add fallback\nif tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n    print(\"üìù Setting pad token properly...\")\n    \n    old_vocab_size = model.config.vocab_size if hasattr(model, 'config') else len(tokenizer)\n    \n    # Method 1: Use eos_token as pad_token\n    if tokenizer.eos_token is not None and tokenizer.eos_token_id is not None:\n        tokenizer.pad_token = tokenizer.eos_token\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n        print(f\"‚úÖ Set pad_token to eos_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n    \n    # Method 2: If eos_token is None or ID is None, add a new pad token\n    elif tokenizer.pad_token_id is None:\n        # Add a new pad token to the tokenizer\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        print(f\"‚úÖ Added new pad_token: [PAD] (ID: {tokenizer.pad_token_id})\")\n    \n    # Method 3: If still None, use unk_token\n    elif tokenizer.unk_token is not None and tokenizer.unk_token_id is not None:\n        tokenizer.pad_token = tokenizer.unk_token\n        tokenizer.pad_token_id = tokenizer.unk_token_id\n        print(f\"‚úÖ Set pad_token to unk_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n    \n    # Method 4: Last resort - use token 0\n    else:\n        tokenizer.pad_token_id = 0\n        tokenizer.pad_token = tokenizer.convert_ids_to_tokens(0)\n        print(f\"‚úÖ Set pad_token_id to 0: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n\n# Force fallback: If pad_token_id is still None after all methods, add [PAD] token\nif tokenizer.pad_token_id is None:\n    print(\"‚ö†Ô∏è Pad token ID still None - forcing addition of [PAD] token...\")\n    old_len = len(tokenizer)\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    if len(tokenizer) > old_len:\n        model.resize_token_embeddings(len(tokenizer))\n    print(f\"‚úÖ Forced added [PAD] token (ID: {tokenizer.pad_token_id})\")\n\n# Verify pad token is properly set (must not be None)\nassert tokenizer.pad_token_id is not None, \"‚ùå Failed to set pad_token_id - it is still None!\"\nprint(f\"üìã After fixing:\")\nprint(f\" - pad_token: {tokenizer.pad_token}\")\nprint(f\" - pad_token_id: {tokenizer.pad_token_id}\")\n\n# Update model config to match tokenizer - Ensure IDs are integers\nprint(\"üîß Updating model config to match tokenizer...\")\nif hasattr(model, 'config'):\n    # Update all token IDs with explicit checks\n    model.config.pad_token_id = int(tokenizer.pad_token_id)\n    model.config.eos_token_id = int(tokenizer.eos_token_id) if tokenizer.eos_token_id is not None else model.config.eos_token_id\n    model.config.bos_token_id = int(tokenizer.bos_token_id) if tokenizer.bos_token_id is not None else model.config.bos_token_id\n    \n    print(\"‚úÖ Model config updated with tokenizer tokens\")\n    print(f\" - Model pad_token_id: {model.config.pad_token_id}\")\n    print(f\" - Model eos_token_id: {model.config.eos_token_id}\")\n    print(f\" - Model bos_token_id: {model.config.bos_token_id}\")\n\n# Also update generation config if it exists\nif hasattr(model, 'generation_config') and model.generation_config is not None:\n    model.generation_config.pad_token_id = int(tokenizer.pad_token_id)\n    model.generation_config.eos_token_id = int(tokenizer.eos_token_id) if tokenizer.eos_token_id is not None else model.generation_config.eos_token_id\n    model.generation_config.bos_token_id = int(tokenizer.bos_token_id) if tokenizer.bos_token_id is not None else model.generation_config.bos_token_id\n    print(\"‚úÖ Generation config updated with tokenizer tokens\")\n    print(f\" - Gen pad_token_id: {model.generation_config.pad_token_id}\")\n    print(f\" - Gen eos_token_id: {model.generation_config.eos_token_id}\")\n    print(f\" - Gen bos_token_id: {model.generation_config.bos_token_id}\")\n\n# Resize model embeddings if new tokens were added (check against original vocab size)\nif tokenizer.pad_token_id is not None and tokenizer.pad_token_id >= old_vocab_size:\n    print(\"üìè Resizing model embeddings for new tokens...\")\n    model.resize_token_embeddings(len(tokenizer))\n    print(\"‚úÖ Model embeddings resized\")\nelif len(tokenizer) > old_vocab_size:\n    print(\"üìè Resizing model embeddings due to vocab size increase...\")\n    model.resize_token_embeddings(len(tokenizer))\n    print(\"‚úÖ Model embeddings resized\")\n\n# Set conservative max sequence length\nmax_seq_length = 512\nprint(f\"üìè Set max_seq_length to: {max_seq_length}\")\n\n# Update training args to match\nif hasattr(training_args, 'max_seq_length'):\n    training_args.max_seq_length = max_seq_length\n\n# Filter dataset aggressively\nprint(\"üîç Filtering dataset...\")\ninitial_size = len(final_dataset)\n\ndef filter_long_sequences(example):\n    \"\"\"Filter out sequences that are too long.\"\"\"\n    text = example.get('text', '')\n    if not text:\n        return False\n    \n    try:\n        tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_seq_length)\n        return len(tokens) <= max_seq_length\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Tokenization error: {e}\")\n        return False\n\nfinal_dataset = final_dataset.filter(filter_long_sequences)\nfiltered_size = len(final_dataset)\nprint(f\"‚úÖ Filtered dataset: {initial_size} -> {filtered_size} examples\")\n\n# Take small subset for testing\nif len(final_dataset) > 50:\n    print(\"üìä Taking small subset for testing...\")\n    final_dataset = final_dataset.select(range(50))\n    print(f\"‚úÖ Reduced to {len(final_dataset)} examples\")\n\n# Truncate sequences\nprint(\"‚úÇÔ∏è Truncating sequences...\")\ndef truncate_sequences(example):\n    \"\"\"Truncate sequences to max length.\"\"\"\n    text = example.get('text', '')\n    if not text:\n        return {'text': ''}\n    \n    try:\n        tokens = tokenizer.encode(\n            text, \n            add_special_tokens=True, \n            truncation=True, \n            max_length=max_seq_length,\n            padding=False\n        )\n        \n        truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n        return {'text': truncated_text}\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Truncation error: {e}\")\n        return {'text': 'Short text example.'}\n\nfinal_dataset = final_dataset.map(truncate_sequences)\nprint(\"‚úÖ Sequences truncated\")\n\n# Check TRL version and SFTTrainer parameters\nprint(\"üîç Checking SFTTrainer parameters...\")\nimport inspect\nsft_params = inspect.signature(SFTTrainer.__init__).parameters\nprint(f\"üìã Available SFTTrainer parameters: {list(sft_params.keys())}\")\n\n# Initialize trainer with correct latest API\nprint(\"üöÄ Initializing SFT Trainer with properly aligned tokenizer...\")\n\ntry:\n    # Latest SFTTrainer API\n    trainer_kwargs = {\n        \"model\": model,\n        \"train_dataset\": final_dataset,\n        \"args\": training_args,\n    }\n    \n    # Add parameters only if they exist in the current API\n    if \"dataset_text_field\" in sft_params:\n        trainer_kwargs[\"dataset_text_field\"] = \"text\"\n    elif \"text_field\" in sft_params:\n        trainer_kwargs[\"text_field\"] = \"text\"\n    \n    if \"max_seq_length\" in sft_params:\n        trainer_kwargs[\"max_seq_length\"] = max_seq_length\n    \n    if \"dataset_num_proc\" in sft_params:\n        trainer_kwargs[\"dataset_num_proc\"] = 1\n    \n    if \"packing\" in sft_params:\n        trainer_kwargs[\"packing\"] = False\n    \n    print(f\"üìã Using parameters: {list(trainer_kwargs.keys())}\")\n    \n    trainer = SFTTrainer(**trainer_kwargs)\n    print(\"‚úÖ Trainer initialized successfully\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Trainer initialization failed: {e}\")\n    print(\"üîÑ Trying with minimal parameters...\")\n    \n    try:\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=final_dataset,\n            args=training_args,\n        )\n        print(\"‚úÖ Trainer initialized with minimal parameters\")\n        \n    except Exception as e2:\n        print(f\"‚ùå Minimal initialization failed: {e2}\")\n        print(\"üîÑ Trying standard Trainer...\")\n        \n        try:\n            from transformers import Trainer\n            \n            trainer = Trainer(\n                model=model,\n                train_dataset=final_dataset,\n                args=training_args,\n            )\n            print(\"‚úÖ Standard Trainer initialized successfully\")\n            \n        except Exception as e3:\n            print(f\"‚ùå Standard Trainer failed: {e3}\")\n            raise e3\n\n# Print training info\nprint(f\"üìã Training configuration:\")\nprint(f\" - Dataset size: {len(final_dataset)}\")\nprint(f\" - Model parameters: {model.num_parameters():,}\")\nprint(f\" - Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\nprint(f\" - Max sequence length: {max_seq_length}\")\nprint(f\" - Training steps: {training_args.max_steps}\")\nprint(f\" - Pad token ID: {tokenizer.pad_token_id}\")\nprint(f\" - EOS token ID: {tokenizer.eos_token_id}\")\n\n# Check memory before training\nprint(f\"üíæ VRAM before training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\nos.system(\"nvidia-smi\")\n\n# Start training\nprint(\"üöÄ Starting fine-tuning...\")\nprint(\"‚è±Ô∏è This may take 10-30 minutes depending on dataset size...\")\n\ntry:\n    # Clear cache before training\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # Start training\n    trainer.train()\n    \n    print(\"‚úÖ Training completed successfully!\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Training failed: {e}\")\n    print(\"üí° Possible solutions:\")\n    print(\" - Check if model and dataset are compatible\")\n    print(\" - Verify VRAM is sufficient\")\n    print(\" - Check dataset format\")\n    print(\" - Try reducing batch size or sequence length\")\n    \n    # Try with reduced parameters\n    print(\"üîÑ Trying with reduced parameters...\")\n    try:\n        # Update training args with smaller values\n        training_args.per_device_train_batch_size = 1\n        training_args.gradient_accumulation_steps = 2\n        training_args.max_steps = 5 # Very small for testing\n        \n        # Reinitialize trainer with reduced parameters\n        if hasattr(trainer, 'args'):\n            trainer.args = training_args\n        \n        print(\"üîÑ Starting training with reduced parameters...\")\n        trainer.train()\n        print(\"‚úÖ Training completed with reduced parameters!\")\n        \n    except Exception as e2:\n        print(f\"‚ùå Reduced training also failed: {e2}\")\n        print(\"üí° Training failed completely. Check:\")\n        print(\" - Model compatibility\")\n        print(\" - Dataset format\")\n        print(\" - Memory availability\")\n        raise e2\n\n# Check memory after training\nprint(f\"üíæ VRAM after training: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\n\n# Save the final model\nprint(\"üíæ Saving final model...\")\ntry:\n    trainer.save_model(\"/kaggle/working/final_model\")\n    print(\"‚úÖ Model saved successfully\")\nexcept Exception as e:\n    print(f\"‚ùå Model saving failed: {e}\")\n    print(\"üí° Model may still be usable despite save failure\")\n\n# Check GPU status\nos.system(\"nvidia-smi\")\nos.system(\"df -h\")\n\nprint(\"\\n‚úÖ Training execution completed!\")\nprint(\"üìã Ready for model saving and upload!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Save Model and Upload to GCS","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Model Saving and GCS Upload (Cell 9)\n# This cell saves the fine-tuned model and uploads it to GCS\n\nimport os\nimport zipfile\nimport shutil\nimport gc\n\n# Import required libraries\ntry:\n    from google.cloud import storage\n    print(\"‚úÖ Google Cloud Storage imported successfully\")\nexcept Exception as e:\n    print(f\"‚ùå GCS import failed: {e}\")\n    os.system(\"pip install google-cloud-storage -q\")\n    from google.cloud import storage\n\n# GCS Configuration\nBUCKET_NAME = 'wizardlm-training-1759276927'\nMODEL_OUTPUT_PATH = 'model_output/wizardlm_fine_tuned'\n\nprint(\"üíæ Starting model saving process...\")\n\n# Save LoRA adapters\nprint(\"üì¶ Saving LoRA adapters...\")\ntry:\n    model.save_pretrained(\"/kaggle/working/lora_adapters\")\n    tokenizer.save_pretrained(\"/kaggle/working/lora_adapters\")\n    print(\"‚úÖ LoRA adapters saved successfully\")\nexcept Exception as e:\n    print(f\"‚ùå LoRA adapter saving failed: {e}\")\n    raise e\n\n# Create a zip file with the model\nprint(\"üì¶ Creating model archive...\")\ntry:\n    zip_path = \"/kaggle/working/wizardlm_fine_tuned.zip\"\n    \n    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(\"/kaggle/working/lora_adapters\"):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, \"/kaggle/working/lora_adapters\")\n                zipf.write(file_path, arcname)\n    \n    # Check zip file size\n    zip_size = os.path.getsize(zip_path) / (1024**3)\n    print(f\"‚úÖ Model archive created: {zip_size:.2f} GB\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Archive creation failed: {e}\")\n    raise e\n\n# Initialize GCS client\nprint(\"‚òÅÔ∏è Connecting to GCS...\")\ntry:\n    client = storage.Client()\n    bucket = client.bucket(BUCKET_NAME)\n    print(f\"‚úÖ Connected to GCS bucket: {BUCKET_NAME}\")\nexcept Exception as e:\n    print(f\"‚ùå GCS connection failed: {e}\")\n    raise e\n\n# Upload to GCS\nprint(\"‚òÅÔ∏è Uploading to GCS...\")\ntry:\n    blob_name = f\"{MODEL_OUTPUT_PATH}/wizardlm_fine_tuned.zip\"\n    blob = bucket.blob(blob_name)\n    \n    # Upload with progress tracking\n    blob.upload_from_filename(zip_path)\n    \n    print(f\"‚úÖ Model uploaded successfully!\")\n    print(f\"üìÅ Location: gs://{BUCKET_NAME}/{blob_name}\")\n    print(f\"üìä File size: {zip_size:.2f} GB\")\n    \nexcept Exception as e:\n    print(f\"‚ùå GCS upload failed: {e}\")\n    raise e\n\n# Verify upload\nprint(\"üîç Verifying upload...\")\ntry:\n    blob = bucket.blob(blob_name)\n    if blob.exists():\n        print(\"‚úÖ Upload verification successful\")\n    else:\n        print(\"‚ùå Upload verification failed\")\nexcept Exception as e:\n    print(f\"‚ùå Upload verification error: {e}\")\n\n# Cleanup local files to save space\nprint(\"üßπ Cleaning up local files...\")\ntry:\n    shutil.rmtree(\"/kaggle/working/lora_adapters\")\n    os.remove(zip_path)\n    print(\"‚úÖ Local files cleaned up\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Cleanup warning: {e}\")\n\n# Force garbage collection\ngc.collect()\n\n# Check disk usage\nos.system(\"df -h\")\n\nprint(\"\\n‚úÖ Model saving and upload completed!\")\nprint(\"üìã Summary:\")\nprint(f\"   - Model saved to: gs://{BUCKET_NAME}/{blob_name}\")\nprint(f\"   - File size: {zip_size:.2f} GB\")\nprint(f\"   - Local files cleaned up\")\nprint(\"\\nüéâ Fine-tuning pipeline completed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Test the Fine-Tuned Model (Optional)","metadata":{}},{"cell_type":"code","source":"# Kaggle-Optimized Model Testing (Cell 10)\n# This cell tests the fine-tuned model with sample prompts\n\nimport torch\nimport os\nimport gc\n\nprint(\"üß™ Testing the fine-tuned model...\")\n\n# Test prompts for different scenarios\ntest_prompts = [\n    {\n        \"name\": \"Python Function\",\n        \"prompt\": \"### Instruction:\\\\nWrite a Python function to calculate the factorial of a number.\\\\n\\\\n### Response:\\\\n\"\n    },\n    {\n        \"name\": \"Code Explanation\", \n        \"prompt\": \"### Instruction:\\\\nExplain what this Python code does:\\\\ndef fibonacci(n):\\\\n    if n <= 1:\\\\n        return n\\\\n    return fibonacci(n-1) + fibonacci(n-2)\\\\n\\\\n### Response:\\\\n\"\n    },\n    {\n        \"name\": \"Algorithm Question\",\n        \"prompt\": \"### Instruction:\\\\nWhat is the time complexity of binary search?\\\\n\\\\n### Response:\\\\n\"\n    }\n]\n\ndef test_model_generation(prompt, max_new_tokens=150, temperature=0.7):\n    \"\"\"Test model generation with a given prompt.\"\"\"\n    try:\n        # Tokenize input\n        inputs = tokenizer(prompt, return_tensors=\"pt\")\n        if torch.cuda.is_available():\n            inputs = inputs.to(\"cuda\")\n        \n        # Generate response\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id\n            )\n        \n        # Decode response\n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract only the generated part\n        generated_text = response[len(prompt):].strip()\n        \n        return generated_text\n        \n    except Exception as e:\n        return f\"‚ùå Generation failed: {e}\"\n\n# Test each prompt\nprint(\"üîç Running model tests...\")\nprint(\"=\" * 60)\n\nfor i, test_case in enumerate(test_prompts, 1):\n    print(f\"\\\\nüìù Test {i}: {test_case['name']}\")\n    print(f\"Prompt: {test_case['prompt'][:100]}...\")\n    print(\"-\" * 40)\n    \n    # Generate response\n    response = test_model_generation(test_case['prompt'])\n    \n    print(f\"Generated Response:\")\n    print(response)\n    print(\"=\" * 60)\n\n# Memory usage check\nprint(f\"\\\\nüíæ Memory usage after testing:\")\nprint(f\"   - VRAM allocated: {torch.cuda.memory_allocated() / 1e9:.2f}GB\")\nprint(f\"   - VRAM cached: {torch.cuda.memory_reserved() / 1e9:.2f}GB\")\n\n# Clean up memory\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Final GPU status\nos.system(\"nvidia-smi\")\n\nprint(\"\\\\n‚úÖ Model testing completed!\")\nprint(\"üìã Test Summary:\")\nprint(\"   - Model responds to different types of prompts\")\nprint(\"   - Generation quality can be assessed from outputs\")\nprint(\"   - Memory usage is within acceptable limits\")\nprint(\"\\\\nüéâ Fine-tuning and testing pipeline completed successfully!\")\nprint(f\"üìÅ Final model available at: gs://{BUCKET_NAME}/{MODEL_OUTPUT_PATH}/wizardlm_fine_tuned.zip\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}