{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Fine-Tuning Uncensored CodeGemma 7B to GGUF f16 (with GCS Option)\n",
    "\n",
    "## Overview\n",
    "Fine-tunes `ICEPVP8977/Uncensored_codegemma_7b` using all specified HF datasets (18 sources merged). Uses Unsloth + 4-bit for low VRAM (~4-6GB). Optional GCS for external storage.\n",
    "\n",
    "**Datasets:** Full merge (~500k-700k examples, ~1.5-2.5GB); sampled 10% default.\n",
    "**Storage:** Fits Kaggle 20GB; use GCS section if needed.\n",
    "**Time:** 1-2h sampled / 4-8h full on T4 GPU.\n",
    "**Output:** `/kaggle/working/fine_tuned_model_f16.gguf` (~14GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies (Unsloth-Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsloth for 2-5x faster, 80% less VRAM\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -q xformers trl peft accelerate bitsandbytes\n",
    "!pip install -q transformers==4.44.2 datasets==2.21.0 huggingface_hub==0.24.6 safetensors==0.4.5\n",
    "\n",
    "# GCS support (optional)\n",
    "!pip install -q google-cloud-storage\n",
    "\n",
    "# GGUF fallback\n",
    "!pip install -q llama-cpp-python\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git /tmp/llama.cpp 2>/dev/null || echo \"llama.cpp already cloned\"\n",
    "%cd /tmp/llama.cpp\n",
    "!make clean && make -j\n",
    "%cd /kaggle/working\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "print(f\"CUDA: {torch.cuda.is_available()}, VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "!nvidia-smi\n",
    "!df -h  # Disk usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Login to Hugging Face (for Gated Datasets)\n",
    "Required for leaderboard datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Paste HF token (e.g., hf_XXX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Load Datasets from HF (Exact Snippets - All 18 Sources)\n",
    "Uses your exact code. Merges all, aligns minimally, samples 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "reference_columns = ['instruction', 'input', 'output']  # Standard for alignment\n",
    "\n",
    "try:\n",
    "    # Exact Step 1: Alpaca uncensored merge (your code verbatim)\n",
    "    dataset_paths = [\n",
    "        \"V3N0M/Jenna-50K-Alpaca-Uncensored\",\n",
    "        \"SaisExperiments/Alpaca-Uncensored\",\n",
    "        \"SaisExperiments/Big-Alpaca-Uncensored\",\n",
    "        \"xzuyn/open-instruct-uncensored-alpaca\",\n",
    "        \"xzuyn/tulu-uncensored-alpaca\",\n",
    "        \"xzuyn/tv-alpaca-open-instruct-uncensored-blend\",\n",
    "        \"dim/dolphin_flan1m_alpaca_uncensored_3k\",\n",
    "        \"dataautogpt3/flan1m-alpaca-uncensored\",\n",
    "        \"ShubhVenom/Uncensored-Alpaca-v01\",\n",
    "        \"V3N0M/Uncensored-Alpaca\",\n",
    "        \"Xennon-BD/Alpaca-uncensored\",\n",
    "        \"VinyVan/flanMini-alpaca-uncensored_bambara\"\n",
    "    ]\n",
    "\n",
    "    # Load the first dataset to get reference columns\n",
    "    dataset1 = load_dataset(dataset_paths[0], split=\"train\")\n",
    "    reference_columns = dataset1.column_names  # Dynamic from first\n",
    "\n",
    "    # Load and select columns for the remaining datasets\n",
    "    datasets = [dataset1]\n",
    "    for path in tqdm(dataset_paths[1:], desc=\"Alpaca Loading\"):\n",
    "        dataset = load_dataset(path, split=\"train\")\n",
    "        dataset = dataset.select_columns(reference_columns)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    # Merge all datasets\n",
    "    alpaca_merged = concatenate_datasets(datasets)\n",
    "\n",
    "    # Print the number of rows in the merged dataset\n",
    "    print(f\"Alpaca lines: {len(alpaca_merged)}\")\n",
    "\n",
    "    # Save the merged dataset to disk\n",
    "    alpaca_merged.save_to_disk(\"merged_uncensored_alpaca\")\n",
    "\n",
    "    # Exact Step 2: Airoboros uncensored (your code)\n",
    "    airoboros1 = load_dataset(\"mrcuddle/airoboros-uncensored\", split=\"train\")\n",
    "    # Minimal align (rename if needed)\n",
    "    if 'prompt' in airoboros1.column_names and 'instruction' not in airoboros1.column_names:\n",
    "        airoboros1 = airoboros1.rename_column('prompt', 'instruction')\n",
    "    if 'completion' in airoboros1.column_names and 'output' not in airoboros1.column_names:\n",
    "        airoboros1 = airoboros1.rename_column('completion', 'output')\n",
    "    airoboros1 = airoboros1.select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Airoboros uncensored: {len(airoboros1)}\")\n",
    "\n",
    "    # Exact Step 3: Airoboros conversation (your code)\n",
    "    airoboros2 = load_dataset(\"mrcuddle/airoboros-uncensored-conversation\", split=\"train\")\n",
    "    # Flatten convos if needed\n",
    "    if 'conversations' in airoboros2.column_names:\n",
    "        def flatten(ex):\n",
    "            instr = ' '.join([t['value'] for t in ex['conversations'] if t['from'] == 'human'])\n",
    "            out = ' '.join([t['value'] for t in ex['conversations'] if t['from'] == 'gpt'])\n",
    "            return {'instruction': instr, 'input': '', 'output': out}\n",
    "        airoboros2 = airoboros2.map(flatten)\n",
    "    airoboros2 = airoboros2.select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Airoboros conversation: {len(airoboros2)}\")\n",
    "\n",
    "    # Exact Step 4: Leaderboard 1 (your code)\n",
    "    leaderboard1 = load_dataset(\"open-llm-leaderboard/details_ehartford__WizardLM-1.0-Uncensored-CodeLlama-34b\",\n",
    "        \"harness_winogrande_5\",\n",
    "        split=\"train\")\n",
    "    def map_lb(ex):\n",
    "        return {'instruction': ex.get('question', ''), 'input': ex.get('context', ''), 'output': ex.get('answer', '')}\n",
    "    leaderboard1 = leaderboard1.map(map_lb).select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Leaderboard winogrande: {len(leaderboard1)}\")\n",
    "\n",
    "    # Exact Step 5: Leaderboard 2 (your code)\n",
    "    leaderboard2 = load_dataset(\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \n",
    "        \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_date_understanding\", split=\"train\")\n",
    "    leaderboard2 = leaderboard2.map(map_lb).select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Leaderboard date_understanding: {len(leaderboard2)}\")\n",
    "\n",
    "    # Exact Step 6: Leaderboard 3 (your code)\n",
    "    leaderboard3 = load_dataset(\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \n",
    "        \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_causal_judgement\", split=\"train\")\n",
    "    leaderboard3 = leaderboard3.map(map_lb).select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Leaderboard causal_judgement: {len(leaderboard3)}\")\n",
    "\n",
    "    # Exact Step 7: Leaderboard 4 (your code)\n",
    "    leaderboard4 = load_dataset(\"open-llm-leaderboard/DevQuasar__DevQuasar-R1-Uncensored-Llama-8B-details\", \n",
    "        \"DevQuasar__DevQuasar-R1-Uncensored-Llama-8B__leaderboard_bbh_boolean_expressions\", split=\"train\")\n",
    "    leaderboard4 = leaderboard4.map(map_lb).select_columns(reference_columns).filter(lambda ex: all(ex.get(col, '') for col in reference_columns))\n",
    "    print(f\"Leaderboard boolean_expressions: {len(leaderboard4)}\")\n",
    "\n",
    "    # Final merge (all 18 sources)\n",
    "    alpaca_merged = load_from_disk(\"merged_uncensored_alpaca\")['train']\n",
    "    all_ds = [alpaca_merged, airoboros1, airoboros2, leaderboard1, leaderboard2, leaderboard3, leaderboard4]\n",
    "    final_dataset = concatenate_datasets(all_ds)\n",
    "    final_dataset = final_dataset.filter(lambda ex: ex['output'] and len(ex['output']) > 10)\n",
    "    final_dataset = final_dataset.shuffle(seed=42)\n",
    "\n",
    "    # Sample 10% (quality preserved; uncomment for full)\n",
    "    sample_size = int(len(final_dataset) * 0.1)\n",
    "    final_dataset = final_dataset.select(range(sample_size))\n",
    "    # final_dataset = final_dataset  # Full for better quality\n",
    "\n",
    "    print(f\"Final merged (all datasets): {len(final_dataset)} examples (~{len(final_dataset)/1000:.1f}k)\")\n",
    "    final_dataset.save_to_disk(\"/kaggle/working/merged_dataset\")\n",
    "    !df -h  # Check disk\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}. Skipping to fallback.\")\n",
    "    final_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\")  # Small fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Optional: Load from GCS Bucket (For External Storage)\n",
    "If HF downloads exceed disk, use GCS. Prep: Upload `key.json` as Kaggle input, create bucket, upload merged datasets there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if using HF (Section 3a). Run this instead for GCS.\n",
    "from google.cloud import storage\n",
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "# Auth: Upload your service account key.json to /kaggle/input/your-dataset/key.json\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/kaggle/input/your-key-dataset/key.json'  # Adjust path\n",
    "\n",
    "client = storage.Client()\n",
    "bucket_name = 'your-gcs-bucket'  # e.g., gs://my-finetune-bucket/\n",
    "local_dir = '/kaggle/working/'\n",
    "\n",
    "# Download merged dataset from bucket (upload your merged_uncensored_alpaca there first)\n",
    "bucket = client.bucket(bucket_name)\n",
    "blobs = bucket.list_blobs(prefix='merged_dataset/')  # Folder in bucket\n",
    "for blob